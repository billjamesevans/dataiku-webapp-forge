import json
import os
import time
from datetime import datetime

import dataiku
import pandas as pd
from flask import Flask, jsonify, request

app = Flask(__name__)

# --- Configuration ---
# Prefer loading config.json from the webapp folder to keep backend.py stable across regenerations.
_EMBEDDED_CONFIG_JSON = r'''{{ {
  "app": {"name": app_name, "subtitle": subtitle or ""},
  "dataiku": project.get("dataiku") or {},
  "transform": project.get("transform") or {},
  "ui": project.get("ui") or {},
  "columns": columns_selected or [],
} | tojson(indent=2) }}'''


def _load_config():
    base_dirs = []
    try:
        base_dirs.append(os.path.dirname(__file__))
    except Exception:
        pass
    base_dirs.append(os.getcwd())

    for base in base_dirs:
        for fname in ("config.json", "app_config.json"):
            path = os.path.join(base, fname)
            try:
                if os.path.isfile(path):
                    with open(path, "r", encoding="utf-8") as f:
                        cfg = json.load(f)
                    if isinstance(cfg, dict):
                        if "columns" not in cfg and "columns_selected" in cfg:
                            cfg["columns"] = cfg.get("columns_selected") or []
                        return cfg
            except Exception:
                continue

    try:
        cfg2 = json.loads(_EMBEDDED_CONFIG_JSON)
        return cfg2 if isinstance(cfg2, dict) else {}
    except Exception:
        return {}


CONFIG = _load_config()
APP = CONFIG.get("app") if isinstance(CONFIG.get("app"), dict) else {}
DATAIKU = CONFIG.get("dataiku") if isinstance(CONFIG.get("dataiku"), dict) else {}
TRANSFORM = CONFIG.get("transform") if isinstance(CONFIG.get("transform"), dict) else {}
UI = CONFIG.get("ui") if isinstance(CONFIG.get("ui"), dict) else {}
COLUMNS = CONFIG.get("columns") if isinstance(CONFIG.get("columns"), list) else []

APP_NAME = str(APP.get("name") or "Dataiku WebApp")
SUBTITLE = str(APP.get("subtitle") or "")
DATASET_A = str(DATAIKU.get("dataset_a") or "dataset_a")

CACHE_TTL_SECONDS = 60
BASE_ROW_LIMIT = int((TRANSFORM or {}).get("limit") or 2000)
BASE_ROW_LIMIT = max(1, min(BASE_ROW_LIMIT, 200000))

_BASE_CACHE = {"at": 0.0, "df": None, "note": ""}


def _dataset_df(name, *, columns=None, limit=None):
    if not name:
        raise ValueError("Missing dataset name.")
    ds = dataiku.Dataset(name)
    kwargs = {"parse_dates": False}
    if columns:
        kwargs["columns"] = list(columns)
    if limit is not None:
        kwargs["limit"] = int(limit)
    try:
        return ds.get_dataframe(**kwargs)
    except TypeError:
        df = ds.get_dataframe()
        if columns:
            keep = [c for c in columns if c in df.columns]
            df = df[keep]
        if limit is not None:
            df = df.head(int(limit))
        return df


def _jsonable(value):
    if value is None:
        return None
    try:
        if pd.isna(value):
            return None
    except Exception:
        pass
    if isinstance(value, (pd.Timestamp, datetime)):
        try:
            return value.isoformat()
        except Exception:
            return str(value)
    try:
        import numpy as np

        if isinstance(value, (np.integer, np.floating, np.bool_)):
            return value.item()
    except Exception:
        pass
    try:
        json.dumps(value)
        return value
    except Exception:
        return str(value)


def _is_blank(series):
    return series.isna() | (series.astype(str).str.strip() == "")


def _mask_for_filter(df, flt):
    col = str(flt.get("column") or "").strip()
    op = str(flt.get("op") or "").strip().lower()
    val = flt.get("value")
    if not col or not op:
        return pd.Series([True] * len(df), index=df.index)
    if col not in df.columns:
        raise ValueError("Filter column not found: " + col)

    s = df[col]

    if op == "blank":
        return _is_blank(s)
    if op == "notblank":
        return ~_is_blank(s)

    if op in {"contains", "contains_cs", "regex", "startswith", "startswith_cs", "endswith", "endswith_cs"}:
        text = s.astype(str)
        q = "" if val is None else str(val)
        if op == "regex":
            return text.str.contains(q, case=False, na=False, regex=True)
        if op == "contains_cs":
            return text.str.contains(q, case=True, na=False, regex=False)
        if op == "contains":
            return text.str.contains(q, case=False, na=False, regex=False)
        if op == "startswith_cs":
            return text.str.startswith(q, na=False)
        if op == "startswith":
            return text.str.lower().str.startswith(q.lower(), na=False)
        if op == "endswith_cs":
            return text.str.endswith(q, na=False)
        return text.str.lower().str.endswith(q.lower(), na=False)

    if op in {"eq", "neq"}:
        q = "" if val is None else str(val)
        mask = s.astype(str) == q
        return mask if op == "eq" else ~mask

    if op in {"in", "notin"}:
        items = [x.strip() for x in str(val or "").split(",") if x.strip()]
        mask = s.astype(str).isin(items)
        return mask if op == "in" else ~mask

    if op in {"gt", "gte", "lt", "lte"}:
        try:
            q = float(str(val).strip())
        except Exception:
            raise ValueError("Numeric filter requires a numeric value for " + col)
        sn = pd.to_numeric(s, errors="coerce")
        if op == "gt":
            return sn > q
        if op == "gte":
            return sn >= q
        if op == "lt":
            return sn < q
        return sn <= q

    if op in {"date_gt", "date_gte", "date_lt", "date_lte"}:
        qd = pd.to_datetime(val, errors="coerce", utc=True)
        sd = pd.to_datetime(s, errors="coerce", utc=True)
        if pd.isna(qd):
            raise ValueError("Date filter requires a parseable date value for " + col)
        if op == "date_gt":
            return sd > qd
        if op == "date_gte":
            return sd >= qd
        if op == "date_lt":
            return sd < qd
        return sd <= qd

    raise ValueError("Unsupported filter operator: " + op)


def _apply_filter_groups(df):
    groups = (TRANSFORM or {}).get("filter_groups")
    if not isinstance(groups, list):
        return df
    groups = [g for g in groups if isinstance(g, dict)]
    groups = [g for g in groups if isinstance(g.get("filters"), list) and g.get("filters")]
    if not groups:
        return df

    keep = pd.Series([False] * len(df), index=df.index)
    for g in groups:
        flts = [f for f in (g.get("filters") or []) if isinstance(f, dict)]
        if not flts:
            continue
        m = pd.Series([True] * len(df), index=df.index)
        for f in flts:
            m = m & _mask_for_filter(df, f)
        keep = keep | m
    return df[keep]


def _apply_computed_columns(df):
    computed = (TRANSFORM or {}).get("computed_columns")
    if not isinstance(computed, list):
        return df
    for cc in computed:
        if not isinstance(cc, dict):
            continue
        ctype = str(cc.get("type") or "").strip()
        name = str(cc.get("name") or "").strip()
        if not ctype or not name:
            continue
        if ctype == "concat":
            cols = cc.get("columns") if isinstance(cc.get("columns"), list) else []
            sep = str(cc.get("sep") or "")
            parts = []
            for c in cols:
                if c not in df.columns:
                    raise ValueError("Computed concat column missing input: " + str(c))
                parts.append(df[c].astype(str).fillna(""))
            df[name] = parts[0] if parts else ""
            for p in parts[1:]:
                df[name] = df[name] + sep + p
        elif ctype == "coalesce":
            cols = cc.get("columns") if isinstance(cc.get("columns"), list) else []
            out = pd.Series([""] * len(df), index=df.index)
            for c in cols:
                if c not in df.columns:
                    raise ValueError("Computed coalesce column missing input: " + str(c))
                s = df[c]
                mask = ~_is_blank(s)
                out = out.where(~mask, s.astype(str))
            df[name] = out
        elif ctype == "date_format":
            col = str(cc.get("column") or "").strip()
            fmt = str(cc.get("format") or "%Y-%m-%d")
            if col not in df.columns:
                raise ValueError("Computed date_format missing input: " + col)
            dt = pd.to_datetime(df[col], errors="coerce", utc=True)
            df[name] = dt.dt.strftime(fmt).fillna("")
        elif ctype == "bucket":
            col = str(cc.get("column") or "").strip()
            size = int(cc.get("size") or 10)
            if col not in df.columns:
                raise ValueError("Computed bucket missing input: " + col)
            if size <= 0:
                raise ValueError("Bucket size must be > 0 for " + name)
            sn = pd.to_numeric(df[col], errors="coerce")
            df[name] = (sn // size) * size
        else:
            raise ValueError("Unsupported computed column type: " + ctype)
    return df


def _needed_columns():
    needed = set()
    computed = (TRANSFORM or {}).get("computed_columns")
    computed_names = set()
    if isinstance(computed, list):
        for cc in computed:
            if isinstance(cc, dict) and cc.get("name"):
                computed_names.add(str(cc.get("name")))

    for c in COLUMNS if isinstance(COLUMNS, list) else []:
        if not isinstance(c, dict):
            continue
        name = str(c.get("name") or "").strip()
        if name and name not in computed_names:
            needed.add(name)

    for g in (TRANSFORM or {}).get("filter_groups") or []:
        if not isinstance(g, dict):
            continue
        for f in g.get("filters") or []:
            if isinstance(f, dict) and f.get("column"):
                needed.add(str(f.get("column")))

    sort_cfg = (TRANSFORM or {}).get("sort") or {}
    sort_col = str(sort_cfg.get("column") or "").strip()
    if sort_col and sort_col not in computed_names:
        needed.add(sort_col)

    ff_cols = (UI or {}).get("frontend_filters") if isinstance((UI or {}).get("frontend_filters"), list) else []
    for col in ff_cols:
        if col:
            needed.add(str(col))

    chart_cfg = (UI or {}).get("chart") or {}
    for k in ("column", "x_column", "y_column"):
        v = str(chart_cfg.get(k) or "").strip()
        if v and v not in computed_names:
            needed.add(v)

    if isinstance(computed, list):
        for cc in computed:
            if not isinstance(cc, dict):
                continue
            ctype = str(cc.get("type") or "").strip()
            if ctype in {"concat", "coalesce"}:
                for c in cc.get("columns") or []:
                    if c:
                        needed.add(str(c))
            elif ctype in {"date_format", "bucket"}:
                c = str(cc.get("column") or "").strip()
                if c:
                    needed.add(c)

    return needed if needed else None


def _build_base_dataframe():
    now = time.time()
    cached = _BASE_CACHE.get("df")
    if cached is not None and (now - float(_BASE_CACHE.get("at") or 0.0)) < float(CACHE_TTL_SECONDS):
        return cached

    needed = _needed_columns()
    df = _dataset_df(DATASET_A, columns=needed, limit=BASE_ROW_LIMIT)
    df = _apply_filter_groups(df)
    df = _apply_computed_columns(df)

    _BASE_CACHE["df"] = df
    _BASE_CACHE["at"] = now
    _BASE_CACHE["note"] = f"Dataset reads capped to first {BASE_ROW_LIMIT} rows for performance."
    return df


def _selected_columns(df):
    cols = [{"name": c.get("name"), "label": c.get("label") or c.get("name")} for c in COLUMNS if isinstance(c, dict)]
    cols = [c for c in cols if c.get("name")]
    if not cols:
        cols = [{"name": c, "label": c} for c in df.columns.tolist()]
    return cols


def _distinct_options(df, col, *, max_items=200):
    if col not in df.columns:
        return []
    s = df[col].dropna()
    try:
        vc = s.astype(str).value_counts(dropna=True)
        vals = [v for v in vc.index.tolist() if str(v).strip() != ""]
        return vals[:max_items]
    except Exception:
        vals = s.astype(str).dropna().unique().tolist()
        vals = [v for v in vals if str(v).strip() != ""]
        vals.sort()
        return vals[:max_items]


@app.route("/rows", methods=["GET"])
def rows():
    try:
        df = _build_base_dataframe()

        filter_cols = (UI or {}).get("frontend_filters") if isinstance((UI or {}).get("frontend_filters"), list) else []
        for col in filter_cols:
            if not isinstance(col, str) or not col:
                continue
            qv = request.args.get("ff_" + col)
            if qv is None or str(qv).strip() == "":
                continue
            if col not in df.columns:
                continue
            df = df[df[col].astype(str) == str(qv)]

        cols = _selected_columns(df)
        missing = [c["name"] for c in cols if c["name"] not in df.columns]
        if missing:
            raise ValueError("Selected columns not found: " + ", ".join(missing))

        names = [c["name"] for c in cols]

        sort_cfg = (TRANSFORM or {}).get("sort") or {}
        sort_col = str(sort_cfg.get("column") or "").strip()
        sort_dir = str(sort_cfg.get("direction") or "asc").strip().lower()
        if sort_col:
            if sort_col not in df.columns:
                raise ValueError("Sort column not found: " + sort_col)
            df = df.sort_values(by=[sort_col], ascending=(sort_dir != "desc"))

        cfg_limit = int((TRANSFORM or {}).get("limit") or 2000)
        cfg_limit = max(1, min(cfg_limit, 200000))
        pagination = bool((UI or {}).get("pagination"))
        page_size = int((UI or {}).get("page_size") or 200)
        page_size = max(10, min(page_size, 5000))

        req_offset = int(request.args.get("offset") or 0)
        req_limit = int(request.args.get("limit") or page_size)
        req_offset = max(0, req_offset)
        req_limit = max(1, min(req_limit, page_size if pagination else cfg_limit))

        total = int(len(df))
        if pagination:
            df_page = df.iloc[req_offset : req_offset + req_limit]
        else:
            df_page = df.head(cfg_limit)
            req_offset = 0
            req_limit = int(len(df_page))

        df_page = df_page[names]

        records = []
        for _, r in df_page.iterrows():
            rec = {}
            for name in names:
                rec[name] = _jsonable(r.get(name))
            records.append(rec)

        filter_options = {c: _distinct_options(df, c) for c in filter_cols if isinstance(c, str) and c}

        chart_cfg = (UI or {}).get("chart") or {}
        chart = None
        if chart_cfg.get("enabled"):
            ctype = str(chart_cfg.get("type") or "bar").strip().lower()
            xcol = str(chart_cfg.get("x_column") or "").strip() or str(chart_cfg.get("column") or "").strip()
            ycol = str(chart_cfg.get("y_column") or "").strip()

            if ctype == "bar":
                top_n = int(chart_cfg.get("top_n") or 12)
                top_n = max(1, min(top_n, 50))
                if xcol and xcol in df.columns:
                    s = df[xcol].astype(str).fillna("").replace({"nan": ""})
                    counts = s[s != ""].value_counts().head(top_n)
                    chart = {"type": "bar", "column": xcol, "labels": counts.index.tolist(), "values": counts.values.tolist()}

            elif ctype == "hist":
                bins = int(chart_cfg.get("bins") or 16)
                bins = max(3, min(bins, 80))
                if xcol and xcol in df.columns:
                    sn = pd.to_numeric(df[xcol], errors="coerce").dropna()
                    if len(sn):
                        counts, _edges = pd.cut(sn, bins=bins, retbins=True, include_lowest=True, duplicates="drop")
                        vc = counts.value_counts().sort_index()
                        labels = [str(i) for i in vc.index.astype(str).tolist()]
                        values = [int(v) for v in vc.values.tolist()]
                        chart = {"type": "hist", "column": xcol, "labels": labels, "values": values, "bins": bins}

            elif ctype == "line":
                agg = str(chart_cfg.get("agg") or "count").strip().lower()
                if xcol and xcol in df.columns:
                    x = df[xcol]
                    xd = pd.to_datetime(x, errors="coerce", utc=True)
                    use_dates = int(xd.notna().sum()) >= max(3, int(0.5 * len(df)))
                    key = xd.dt.date if use_dates else x.astype(str)

                    if agg == "count" or not ycol:
                        ser = df.groupby(key).size()
                    else:
                        if ycol not in df.columns:
                            ser = df.groupby(key).size()
                            agg = "count"
                        else:
                            yn = pd.to_numeric(df[ycol], errors="coerce")
                            if agg == "sum":
                                ser = yn.groupby(key).sum(min_count=1)
                            elif agg == "mean":
                                ser = yn.groupby(key).mean()
                            else:
                                ser = df.groupby(key).size()
                                agg = "count"

                    ser = ser.dropna().sort_index().tail(60)
                    labels = [str(x) for x in ser.index.tolist()]
                    values = [float(v) if v is not None else 0 for v in ser.values.tolist()]
                    chart = {"type": "line", "x": xcol, "y": (ycol or agg), "labels": labels, "values": values}

            elif ctype == "scatter":
                max_points = int(chart_cfg.get("max_points") or 600)
                max_points = max(50, min(max_points, 5000))
                if xcol and ycol and xcol in df.columns and ycol in df.columns:
                    xn = pd.to_numeric(df[xcol], errors="coerce")
                    yn = pd.to_numeric(df[ycol], errors="coerce")
                    tmp = pd.DataFrame({"x": xn, "y": yn}).dropna()
                    if len(tmp) > max_points:
                        tmp = tmp.sample(n=max_points, random_state=0)
                    pts = [{"x": float(r.x), "y": float(r.y)} for r in tmp.itertuples(index=False)]
                    chart = {"type": "scatter", "x": xcol, "y": ycol, "points": pts, "max_points": max_points}

        return jsonify(
            {
                "status": "ok",
                "rows": records,
                "total": total,
                "offset": req_offset,
                "limit": req_limit,
                "meta": {
                    "app_name": APP_NAME,
                    "subtitle": SUBTITLE,
                    "dataset_a": DATASET_A,
                    "columns": cols,
                    "ui": UI,
                    "filter_options": filter_options,
                    "chart": chart,
                    "note": str(_BASE_CACHE.get("note") or ""),
                },
            }
        )
    except Exception as exc:
        return jsonify({"status": "error", "message": str(exc)}), 500
